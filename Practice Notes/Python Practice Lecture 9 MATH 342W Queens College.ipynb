{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Practice Lecture 9 MATH 342W Queens College - OLS Using Categorical Predictors\n",
    "## Author: Amir ElTabakh\n",
    "## Date: March 1, 2022\n",
    "\n",
    "## Agenda:\n",
    "* OLS using categorical predictors\n",
    "\n",
    "## OLS Using Categorical Predictors\n",
    "\n",
    "Note that historically this is called \"Analysis of Variance\" or \"ANOVA\" for short. But there is no difference to the computer, it still crunches the same matrices.\n",
    "\n",
    "Let's get the cars93 data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import numpy as np # mathematical operations\n",
    "import pandas as pd # pandas DataFrame object\n",
    "from sklearn.linear_model import LinearRegression # Build Linear Regression models\n",
    "import statsmodels.api as sm # Get standard R datasets\n",
    "from sklearn.metrics import mean_squared_error, r2_score # RMSE, R^2\n",
    "\n",
    "# Load dataset\n",
    "cars = sm.datasets.get_rdataset(\"Cars93\", \"MASS\")\n",
    "\n",
    "# Assign data to a variable as a df object\n",
    "cars_df = pd.DataFrame(cars.data)\n",
    "cars_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to model `Type`, a factor with 6 levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out categories\n",
    "cars_df['Type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will $\\hat{y}$ look like? Should be the $\\bar{y}$'s for each level. What is $p$? 6. First we'll use the `pandas.get_dummies` method to convert the categorical variables into dummy/indicator variables. Regression results are easiest to interpret when dummy variables are limited to two specific values, 1 or 0. Typically, 1 represents the presence of a qualitative attribute, and 0 represents the absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X\n",
    "X = cars_df[['Type']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummify categorical variables\n",
    "X = pd.get_dummies(data=X, drop_first=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one categorical variable got blown up into 5 features. How to interpret? First need to know the \"reference category\" i.e. which level is missing in the list. We can see from cross-referencing the coefficient names with the table of the raw feature that the reference category is `Compact`. So what is prediction for the compact type? The intercept. What is prediction of Large type? Intercept + Large, etc. We do not need to add a column of 1's to generate an intercept value.\n",
    "\n",
    "Now let's build a linear model and get the coefficients and intercept then get the $R^2$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y\n",
    "y = cars_df[['Price']]\n",
    "\n",
    "# initialize model\n",
    "anova_model = LinearRegression()\n",
    "\n",
    "# fit model\n",
    "anova_model.fit(X, y)\n",
    "\n",
    "# print b0\n",
    "print(anova_model.intercept_)\n",
    "\n",
    "# print coefficients\n",
    "print(anova_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2\n",
    "print(anova_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model matrix. We'll calculate our $R^2$ via the theory we learn in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert intercept\n",
    "X.insert(0, 'Intercept', [1 for i in range(len(X))])\n",
    "\n",
    "# Convert df to matrix\n",
    "X_m = X.to_numpy()\n",
    "\n",
    "# Print first 10 rows of X_m\n",
    "X_m[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X transpose\n",
    "Xt = X_m.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X transpose * X\n",
    "XtX = Xt @ X_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XtX Inverse\n",
    "XtX_inv = np.linalg.inv(XtX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve for b\n",
    "b = XtX_inv @ Xt @ y\n",
    "\n",
    "# Rename column name\n",
    "b.columns = ['b vector']\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get yhat values\n",
    "yhat = (X_m @ b).to_numpy()\n",
    "yhat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define residual error\n",
    "e = (y - yhat).to_numpy()\n",
    "e[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2\n",
    "Rsq = float((np.var(y) - np.var(e)) / np.var(y))\n",
    "Rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "np.sqrt(sum(e**2) / (len(X) - 6))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course the coefficients and $R^2$ are identical to the output from `lm`.\n",
    "\n",
    "If we want to do a more \"pure ANOVA\", we can get rid of the intercept and see the $\\bar{y}$'s immediately. This is handled when you initialize the model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y\n",
    "y = cars_df[['Price']]\n",
    "X = pd.get_dummies(data=cars_df[['Type']], drop_first=False)\n",
    "\n",
    "# initialize model\n",
    "anova_model = LinearRegression(fit_intercept = False)\n",
    "\n",
    "# fit model\n",
    "anova_model.fit(X, y)\n",
    "\n",
    "# print b0\n",
    "print(anova_model.intercept_)\n",
    "\n",
    "# print coefficients\n",
    "print(anova_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df.groupby('Type').mean()['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does $R^2$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 no intercept\n",
    "anova_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember this from last time? What happened? The $R^2$ calculation in `lm` is not accurate without the intercept. Keep this in mind. \n",
    "\n",
    "What does the design matrix (model matrix) look like? we can use the `.ro_numpy()` function to generate the columns of $X$ from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df to matrix\n",
    "X_m = X.to_numpy()\n",
    "X_m[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressions without an intercept are not recommended. Here's why. What if we were doing two factors? I want a linear model with both Type and Airbags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring AirBags column\n",
    "print(cars_df['AirBags'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AirBags is another nominal categorical variable, this time with three levels.\n",
    "\n",
    "We invoke the model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y\n",
    "y = cars_df[['Price']]\n",
    "X = pd.get_dummies(data=cars_df[['Type', 'AirBags']], drop_first=True)\n",
    "\n",
    "# X column names\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "anova_model = LinearRegression(fit_intercept = True)\n",
    "\n",
    "# fit model\n",
    "anova_model.fit(X, y)\n",
    "\n",
    "# print b0\n",
    "print(anova_model.intercept_)\n",
    "\n",
    "# print coefficients\n",
    "print(anova_model.coef_)\n",
    "\n",
    "# get yhat\n",
    "yhat = anova_model.predict(X)\n",
    "\n",
    "# print R^2\n",
    "print(f\"R Squared: {r2_score(y, yhat)}\")\n",
    "\n",
    "# print RMSE\n",
    "print(f\"RMSE: {mean_squared_error(y_true=y, y_pred=yhat, squared=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get yhat\n",
    "yhat = anova_model.predict(X)\n",
    "\n",
    "# Calculating RMSE\n",
    "rmse = mean_squared_error(y_true=y, y_pred=yhat, squared=False)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are interpretations now? What is the \"reference level\"? It's actually two levels in one: Type = compact and Airbags = Driver \\& Passenger. \n",
    "\n",
    "A deeper question: can we read off Type = Midsize and AirBags = none? No... this is a modeling \"enhancement\" we will discuss in a few lectures from now.\n",
    "\n",
    "If we model it without an intercept,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y\n",
    "y = cars_df[['Price']]\n",
    "X = pd.get_dummies(data=cars_df[['Type', 'AirBags']], drop_first=False)\n",
    "\n",
    "# X column names\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "anova_model = LinearRegression(fit_intercept = False) # not modeling intercept\n",
    "\n",
    "# fit model\n",
    "anova_model.fit(X, y)\n",
    "\n",
    "# print b0\n",
    "print(anova_model.intercept_)\n",
    "\n",
    "# print coefficients\n",
    "print(anova_model.coef_)\n",
    "\n",
    "# get yhat\n",
    "yhat = anova_model.predict(X)\n",
    "\n",
    "# print R^2\n",
    "print(f\"R Squared: {r2_score(y, yhat)}\")\n",
    "\n",
    "# print RMSE\n",
    "print(f\"RMSE: {mean_squared_error(y_true=y, y_pred=yhat, squared=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only get $\\bar{y}$'s for the first factor predictor crossed with the reference category of the second. So above `TypeCompact` refers to the average of Type = Compact and Airbags = Driver \\& Passenger.\n",
    "\n",
    "Now let's create a linear model using one categorical predictor and one continuous predictor. The combination is called for historical reasons \"Analysis of Covariance\" or \"ANCOVA\" for short.\n",
    "\n",
    "Let's use `Type` and `Horsepower`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y\n",
    "y = cars_df[['Price']]\n",
    "X = pd.get_dummies(data=cars_df[['Type', 'Horsepower']], drop_first=True)\n",
    "\n",
    "# X column names\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "ancova_model = LinearRegression(fit_intercept = True) # not modeling intercept\n",
    "\n",
    "# fit model\n",
    "ancova_model.fit(X, y)\n",
    "\n",
    "# print b0\n",
    "print(ancova_model.intercept_)\n",
    "\n",
    "# print coefficients\n",
    "print(ancova_model.coef_)\n",
    "\n",
    "# get yhat\n",
    "yhat = ancova_model.predict(X)\n",
    "\n",
    "# print R^2\n",
    "print(f\"R Squared: {r2_score(y, yhat)}\")\n",
    "\n",
    "# print RMSE\n",
    "print(f\"RMSE: {mean_squared_error(y_true=y, y_pred=yhat, squared=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of estimated coefficients? Why did $R^2$ increase? (We will be explaining this in detail in the next unit).\n",
    "\n",
    "What's going on in the design / model matrix? Note that there is an additional column vector with 1's that we account for in the model intialization line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as model matrix with just `Type`. Since `Horsepower` is continuous, it doesn't get dummified to more features.\n",
    "\n",
    "What if we went back to the `Type` regression, left out the intercept, dummified and added the intercept back in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y\n",
    "y = cars_df[['Price']]\n",
    "X = pd.get_dummies(data=cars_df[['Type']], drop_first=False)\n",
    "\n",
    "# X column names\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "ancova_model = LinearRegression(fit_intercept = False) # not modeling intercept\n",
    "\n",
    "# fit model\n",
    "ancova_model.fit(X, y)\n",
    "\n",
    "# print b0\n",
    "print(ancova_model.intercept_)\n",
    "\n",
    "# print coefficients\n",
    "print(ancova_model.coef_)\n",
    "\n",
    "# get yhat\n",
    "yhat = ancova_model.predict(X)\n",
    "\n",
    "# print R^2\n",
    "print(f\"R Squared: {r2_score(y, yhat)}\")\n",
    "\n",
    "# print RMSE\n",
    "print(f\"RMSE: {mean_squared_error(y_true=y, y_pred=yhat, squared=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's derive the coefficients ourselves,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df to matrix\n",
    "X = X.to_numpy()\n",
    "\n",
    "XtX = X.transpose() @ X\n",
    "\n",
    "XtX_inverse = np.linalg.inv(XtX)\n",
    "\n",
    "b = XtX_inverse @ X.transpose() @ y\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT CONSISTENT WITH KAPS R NOTES. The matrix is invertible and works fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
